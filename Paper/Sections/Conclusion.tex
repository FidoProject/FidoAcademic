A general robotic control system nicknamed Fido was developed that learned tasks with limited feedback.
Fido couples the training of artificial neural networks with a wire-fitted moving least squares interpolator to achieve a continuous state-action space $Q$-learning reinforcement algorithm implementation.
Fido leverages a Boltzmann distribution of probability based on reward to select actions, allowing it to continuously explore its state-action space.
A kinematically accurate robot was simulated with a differential drive system, a sensor array, and other outputs for testing and evaluation purposes.
The robot was trained on a number of common robotic tasks and successfully converged on these tasks in very few reward iterations while maintaining impressively low latency.
Next a hardware implementation was built using a differential drive system, capable of utilizing real world sensor inputs to learn tasks such as driving to a point and staying still.
In the future, we hope to further improve Fido's software and consider further applications in the present.
