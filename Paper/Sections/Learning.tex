\subsection{Wire-Fitted Q-Learning}

The $Q$-Learning algorithm had to be immediately modified to be able to work in continuous state-action spaces for it to be suitable for Fido.
Conventional $Q$-Learning is discrete.
No relation is made between states or actions, and every action for each state must be performed individually in a noisy feedback system to determine its $Q$-value.
However, Fido will work in a large, continuous state-action spaces where relations made between (state-action, $Q$-value) pairs can drastically reduce the number of reward iterations needed for convergence.
An example of a task that would benefit from continuity is teaching Fido to adjust the speed of its motors based on the intensity of light that the robot detects.
There is an obvious gradient relation between the (state-action, $Q$-value) pairs in this task and with a limited number of $Q$-values known, it is possible to correctly model $Q$.

To accommodate continuous action-spaces, we coupled a wire-fitted moving least squares interpolator with our feed-forward neural network as described in \cite{gaskett}.

Feed-forward neural networks can generalize between states in $Q$-Learning problems with discrete actions as described in \cite{rummery}.
To extend this implementation to a continuous action space, our feed-forward neural network outputs discrete ``wires'' when given a state.
Each wire consists of an action with its respective $Q$-value for the state given to the neural network.
These wires may be interpolated to model $Q$, allowing us to get the $Q$-value of any action performed in a state given as an input to the network.
The interpolator used in Fido is a wire-fitted moving least squares interpolator used in the context of a memory-based learning system \cite{baird}.

The wire-fitting function calculates $Q$-value of an action $\hat{a}$ for a state $s$ given a set of $n$ actions $a$ each with a respective $Q$-value $q$ as such:

\begin{equation}
	Q(a, s) = \cfrac{\sum_{i=0}^{n}\cfrac{q_i}{||\hat{a}-a_i||^2+c(q_{max}-q_i)+k}}{\sum_{i=0}^{n}\cfrac{1}{||\hat{a}-a_i||^2+c(q_{max}-q_i)+k}}
	\,,
\end{equation}

\noindent
where $q_{max}$ is the greatest $Q$-value among the set of $Q$-values $q$, and $k$ is a small value that avoids division by zero.
$c$ is the smoothing factor.
The greater the smoothing factor, the smoother the interpolated function.


Figure \ref{fig::wirefitexample} is an example of interpolation on a set of wires.
The graph shows the value of one-dimensional actions plotted against their respective $Q$-values.
The wire-fitting function has few properties that make it especially suited for Fido.

\begin{figure}[ht]
   \centering
   \includegraphics[height=6.5cm]{Figures/WireFit.png}
	\caption{Moving Least Squares Interpolator (adapted from Gaskett, Wettergreen, \& Zelinsky, 1999)}
   \label{fig::wirefitexample}
\end{figure}

Every update to the $Q$-value requires that $q_{max}$ is computed and the action that produces $q_{max}$ is needed for common action selection policies.
As proved in \cite{baird}, the wire with the greatest $Q$-value is the interpolation point with the greatest $Q$-value, therefore $q_{max}$ is the maximum $Q$-value out of the set of wires given to the wire-fitting function.
This makes it extremely computationally cheap to compute $Q$-value, allowing Fido's latency to stay minimal.

The wire-fitting function is derivable.
This allows us to update our wires, and therefore our model of $Q$, using gradient descent.
Gradient descent is an optimization algorithm that looks to find the local minimum of a function by modifying each of its parameters, one by one.
The update function for a parameter is calculated as such:

\begin{equation}
	a = a - \gamma \Delta F(a)
	\,,
	\label{equ::wirefiterrorfunction}
\end{equation}

where $a$ is the parameter to be updated, $\gamma$ is the learning rate, and $\Delta F(a)$ is the partial derivative of the function to be minimized $F$ with respect to $a$.
In the case of Fido, once the reward and new state for an action-state pair is received and an updated $Q$-value $\hat{q}$ is calculated using Equation \ref{equ::updateqlearn}, we are trying to minimize the wire-fitting function's error at predicting $\hat{q}$ when given the wires for Fido's previous state.
This error can be calculated as:

\begin{equation}
	(\hat{q} - q)^2
	\,,
	\label{equ::wirefiterrorfunction}
\end{equation}

\noindent

where $q$ is the old $Q$-value.
Using Equation \ref{equ::wirefiterrorfunction} as our function to be minimized and the partial derivative of the wire-fitting function with respect to each action vector and each q-value, we may compute the partial derivatives of our cost function.
Using these, new wires may be calculated for Fido's previous state using gradient descent.

Fido's neural network may be trained is trained to output these wires using the Adadelta training algorithm. Adadelta is a variant of backpropgation that dynamically adjusts its learning rate to an optimum value, allowing for lower latency. This increases the universality of Fido, since backpropagation's learning rate is task specific.

\subsection{Uncertainty Value}

Fido continuously calculates the "uncertainty" $u$ of its system. Such a value should lower as Fido learns a task and rise if Fido's task changes. This value is useful in adjusting a number hyperparameters that are detailed in the next few sections, such as the exploratory level of the system. After receiving reward and updating its wires using stochastic gradient descent, Fido calculates its uncertainty value as the means squared error between its original set of wires and its newly updated set of wires. In this way, Fido uncertainty value is proportional to the disparity between the current predictions of its model and new information presented to Fido during the current learning iteration.

\subsection{Action Selection}

$Q$-learn requires that actions are selected to be performed.
There are a number of approaches to choosing this action, and each has a large affect on the behavior of the learning implementation.
The most common approach is to simply pick the action with the best $Q$-value for the current state.
However this strategy stifles exploration of the state-action space, increasing the time of convergence on a task, hurting the retrain-ability of the model, and giving a bias towards an actor's starting policy, which is random.
This problem is compounded in the case of Fido due to the large state and action spaces that Fido must explore.
Another common method of action selection is to choose each action randomly for a set number of reward iterations, and then to switch to choosing the action with the highest $Q$-value.
This plan improves upon the first by allowing for a period of exploration, but is not suited for Fido.
During its lifetime, Fido must have the ability to learn new tasks and be retrained by the operator providing feedback, and so, must continuously explore its state space.

Fido selects actions probabilistically using a Boltzmann or soft-max distribution of probability.
The likelihood that an action $\hat{a}$ will be chosen from a set of $n$ actions $a$ for a state $s$ is given as:

\begin{equation}
	p(\hat{a}) = \cfrac{e^{\frac{Q(s, \hat{a})}{T}}}{\sum_{i=0}^{n}e^{\frac{Q(s, a_i)}{T}}}
	\,.
	\label{equ::boltzmann}
\end{equation}

$T$ is the temperature, or exploration level.
As $T$ approaches infinity, a random action is chosen.
As $T$ tends toward 0, the best action is chosen.
Fido keeps $T$ at a constant value around $T \approx 0.15$ throughout its lifetime to encourage occasional, continued exploration.

$T$ is set proportional to Fido's uncertainty value, so that Fido rapidly explores when it is first initialized or is being retrained. This allows Fido to converge on tasks in fewer reward iterations than if $T$ were set as a constant. As Fido learns a task, $T$ will approach 0. This means that Fido will accurately perform a task when sure of its actions, increasing Fido's average reward over other systems.

\subsection{Experience Replay}

As a reinforcement learning system proceeded throughout a problem, the effect of former learning iterations decreases with time as the system updates the parameters of its model to lower its error on new learning iterations. This is wasteful since some iterations will present the system with critical information, such as tagging an unlikely action with high reward.

Fido stores each learning iteration's initial and final states, action, and reward, collectively known as an "experience." Every learning iteration, Fido trains its model on a few past experiences, which are sampled probabilistically according to their age. Weight is given to newer experiences. The number of experiences sampled is proportional to the inverse of Fido's uncertainty value, as past histories will become invalid if the parameters of a problem change. This practice, called "experience replay," lowers the number of learning iterations required for convergence by utilizing all of the information available to Fido.

\subsection{Dynamic Model Architecture}

The optimal size of a neural network is directly related to the complexity of the function that such a neural network has to model. This requires that Fido dynamically size its network, since a constant neural network architecture would never be able to converge on some tasks. Likewise, Fido must dynamically optimize the number of wires outputted by its neural network, since more complicated tasks require a larger number wires to effectively model their action-reward function.

The most accurate method of model architecture optimization would be a brute force search. When using such a method, all the possible permutations of Fido's model within a range would be generated. Each possible model would be trained on a sample of Fido's experiences for a finite number of epochs. The model with the lowest error on the sample of experiences would be selected as Fido's current model.

However, such a method is extraordinarily latent and since Fido needs to be able to run on low power systems, is infeasible. Stochastic gradient descent would be a natural choice for such a search, but the relationship between Fido's error and its model architecture is not easily derivable.

Instead, Fido performs a simultaneous perturbation stochastic approximation to adjust its model architecture. SPSA is gradient descent method designed for discrete systems that requires only two samples of the cost function to compute its derivative. Its update function is similar to \ref{equ::wirefiterrorfunction} and is given as:

\begin{equation}
	u_{n+1} = u_n - a_n\hat{g_n}(u_n)
	\label{equ::spsaupdate}
\end{equation}

\noindent

where $n$ is the current epoch; $u$ is our model parameters, the number of wires and neurons; $a$ is a positive sequence of numbers that approach 0 as $n$ approaches 0; and $\hat{g_n}$ is an estimation of the derivative of the cost function. SPSA defines $\hat{g_n}$ as:

\begin{equation}
	\hat{g_n}(u_n) = \cfrac{J(u_n + c_n\Delta_n)-J(u_n - c_n\Delta_n)}{2c_n(\Delta_n)_i}
	\,.
	\label{equ::spsaderivative}
\end{equation}

\noindent

where $\Delta_n$ is a random perturbation vector and $J$ is the cost function. For Fido, $J$ is the error of the the model $m_n$ that is defined by $u_n$ on a sample of experiences after a period of training.

To optimize the sampling of $J$, Fido derives $m_n$'s neural network from the Fido's current, already trained network. When $m_n$'s neural network is smaller than the current network, Fido derives $m_n$'s neural network by applying a pruning algorithm to its current neural network. This decreases the starting error of $m_n$. When $m_n$'s neural network is larger than the current network, new neurons to Fido's current neural network are added randomly to derive $m_n$'s neural network.
