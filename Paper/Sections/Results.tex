\subsection{Results in Simulation}

To test Fido's effectiveness at learning with limited feedback, Fido was first trained on a number of different tasks through our simulator using reward values delegated through software.
Data was collected regarding Fido's latency and number of reward iterations needed for convergence.

Fido's first and simplest task, dubbed ``Flash,'' was to set the brightness value of an LED to a value proportional to the amount of light that Fido sensed.
Each reward iteration, Fido's neural network was given the intensity of visible light that Fido detected and was asked for the brightness value of Fido's LED.
Fido was then given a reward value equal to $1 - |b - v|$ where $b$ was the brightness value of Fido's LED ranging from 0 to 1 and $v$ was the intensity of visible light that Fido detected ranging from 0 to 1.

``Float,'' Fido's second task, challenged our learning implementation to direct a robot to point.
Each time it was told to select an action, Fido specified the robot's vertical and horizontal velocity between +30 and -30 pixels.
This emulates a holonomic drive systems, where motor outputs directly correlate to movement on the $x$ and $y$ axes.
At the start of each trial, Fido and the point were placed randomly on a boundless plane within 768 pixels of one another.
Fido was fed the ratio of its $x$ displacement to its $y$ displacement from its target point as the state.
Reward was calculated as the difference between Fido's distance away from the target point before performing the action and Fido's distance from the target point after performing the action.
Fido completed each trial when it was within 60 pixels of the point.

Fido's next task, nicknamed ``Drive,'' required that it direct a robot to point by controlling the motors of a differential drive system.
At the start of each trial, Fido and the point were placed randomly on a boundless plane within 768 pixels of one another.
Fido was fed the ratio of its $x$ displacement to its $y$ displacement from its target point as well as its rotation.
Reward was calculated as the difference between Fido's distance away from the target point before performing the action and Fido's distance from the target point after performing the action.
Fido completed each trial when it was within 60 pixels of the point.

Fido's last challenge, called ``Follow,'' was to perform the classic robotic task of line following by controlling the motors of the simulator's differential drive system.
At the start of each trial, Fido was placed on  a line with a random rotation.
Fido was fed the ratio of its $x$ displacement to its $y$ displacement from the closest point on the line as well as its rotation.
Reward was calculated as the difference between Fido's distance away from the line before performing the action and Fido's distance from the line after performing the action.
Fido completed each trial when it had stayed within 60 pixels of the line for 10 consecutive actions.

\subsubsection{Simulation Findings}

Each task was run 400 times to gather the data show in Table \ref{tab:data}.
The reward iterations values shown in the above data table were the medians of the data collected.
The median is shown instead of the mean to discount a few large outliers that were present in data.
The time data shown above is the mean of the data collected.

\begin{table}[ht]
	\centering
	\begin{tabular}{@{}lccc@{}}
		\toprule
		Task        & Learning Iterations & Action Selection (ms) & Training Time (ms) \\ \midrule
		Flash       & 6                   & 0.                 & 28                    \\
		Float       & 14                  & 1                  & 63                    \\
		Drive       & 16                  & 1                  & 94                    \\
		Line Follow & 10                  & 0.                 & 90.				   \\ \bottomrule
	\end{tabular}
	\caption{Number of Learning Iterations, Action Selection Time, and Training Time Per Iteration for Fido Simulation Tasks}
	\label{tab:data}
\end{table}

The data collected through the simulator demonstrates the prowess of the Fido control system with computer-delegated reward in multiple configurations and situations.
Fido was able to master both a holonomic and differential-drive motor control system (the ``Float'' and ``Drive'' tasks), proving its hardware agnostic capabilities.
All tasks showed low numbers of reward iterations and low latency, allowing Fido to learn quickly and efficiently.
The task that was most difficult for Fido, ``Drive,'' took a median of 16 reward iterations, well within the patience of a human.

\subsection{Results in Hardware}

Data was next collected from Fido's hardware implementations to gage performance with human given feedback in real world situations.
Reward was administered through the mobile application previously discussed.  25 trials were done for each task to gather the data shown in the below tables.  As with the simulation, reward iterations values shown are the medians of the data collected while time data shown is the mean of the data collected.

\subsubsection{Thing One Results}

The first task to be given to Thing One was to stay still.
As one would expect, Fido's sole responsibility for this task was not to move.
Fido was administered positive reward when it didn't move, and negative reward when it did.
Next, Thing One was tasked with driving to a point.
At the start of each trial, Fido and the point were placed randomly on a smooth, hardwood surface within 0.75 meters of one another.
Fido was told the ratio of its $x$ displacement to its $y$ displacement from its target point as well as its rotation.
Every fourth action that Fido made was chosen as a reward iteration, and Fido was given a reward value corresponding to its last action.
This reward value was chosen by the tester based on whether Fido moved toward the point or not.
Fido completed each trial when it was within 10 cm of the point.


\begin{table}[ht]
	\centering
	\begin{tabular}{@{}lccc@{}}
		\toprule
		Task             & Learning Iterations & Action Selection (ms) & Training Time (ms) \\ \midrule
		Stay Still       & 3                   & 1                    & 43.5                  \\
		Drive to Point   & 18                  & 4                     & 65                  \\
	\end{tabular}
	\caption{Number of Learning Iterations, Action Selection Time, and Training Time Per Iteration for Thing One Tasks}
	\label{tab:data2}
\end{table}

Performance was slower on the hardware implementation than in simulation for the drive to point task due to the limited computation power of the Intel Edison compute model compared to modern desktop computers.
However, the results still demonstrated Fido's real world applicability through its ability to perform outside of a simulation environment.

\subsubsection{Thing Two Results}

The second hardware implementation constructed, nicknamed ``Thing Two,'' was tasked to master its complex holonomic drive system and numerous sensors to master tasks with limited preprocessing.
The first task administered asked Fido to drive straight using its three 90$^{\circ}$ Swedish wheels, requiring Fido to determinethe correct ratio of motor values outputted.
Fido was administered reward somewhat subjectively by a human operator, being given positive reward when it drove in a less lopsided fashion.
Next, Fido was tasked with following a line.
Fido's line sensor reported whether it was to the left or right of a line.
A course was laid out using black electrical tape for Thing Two to learn on.
Training was performed by placing Thing Two to the left or right of a line.
If the robot moved towards the line, it was administered positive feedback.
If the robot moved away from the line, it would receive negative feedback.
Fido was also trained to follow a ball using its ZX sensor, which reported the location of an object in front of it in the horizontal and vertical dimensions.
Similiarly, Fido was administered positive reward if it drove towards the ball, and negative reward if it drove away.
Finally, Fido's retrainability and applicability was tested in a task that involved loading a pretrained line following model, unplugging a motor, and retraining the robot to compensate.

\begin{table}[ht]
	\centering
	\begin{tabular}{@{}lccc@{}}
		\toprule
		Task              & Learning Iterations & Action Selection (ms) & Training Time (ms) \\ \midrule
		Drive Straight         & 13             & 2                     & 215                \\
		Line Following         & 15             & 21                    & 398                \\
		Fetch                  & 8              & 1                     & 246                \\
		Limping Line Following & 6              & 20                    & 464                \\ \bottomrule
	\end{tabular}
	\caption {Number of Learning Iterations, Action Selection Time, and Training Time Per Iteration for Thing Two Tasks} \label{tab:thingtworesults}
\end{table}

\subsubsection{Thing Three Results}

The last hardware implementation constructed was a four axis robotic arm, nicknamed ``Thing Three.''
Thing Three was first tasked with drawing a square.
As output, Fido could choose from a set of ``waypoints,'' predefined sets of angle positions representing important positions in the operating enviroment.
As input, Fido was given its current waypoint.
If Fido drew a correct line in the sequence of drawing a square, it was given positive reward.
Next, Thing Three was trained to hit a ball with a paddle (or ``play ping pong'') using its two ultrasonic sensors to sense ball position.
Fido was able to adjust its joint rotations as outputs to hit the ball.
In training, positive feedback was administered if Thing Three hit the paddle towards the ball, while negative feedback was given if Thing Three moved in the opposite direction.

\begin{table}[ht]
	\centering
	\begin{tabular}{@{}lccc@{}}
		\toprule
		Task              & Learning Iterations & Action Selection (ms) & Training Time (ms) \\ \midrule
		Draw Square         & 5                   & 5                    & 40                 \\
		Ping Pong          & 8                  & 3                    & 379                \\
		\bottomrule
	\end{tabular}
	\caption {Number of Learning Iterations, Action Selection Time, and Training Time Per Iteration for Thing Three Tasks} \label{tab:thingthreeresults}
\end{table}
