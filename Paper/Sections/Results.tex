\subsection{Results in Simulation}

To test Fido's effectiveness at learning with limited feedback, Fido was first trained on a number of different tasks through our simulator using reward values delegated through software.
Data was collected regarding Fido's latency and number of reward iterations needed for convergence.

Fido's first and simplest task, dubbed ``Flash,'' was to set the brightness value of an LED to a value proportional to the amount of light that Fido sensed.
Each reward iteration, Fido's neural network was given the intensity of visible light that Fido detected and was asked for the brightness value of Fido's LED.
Fido was then given a reward value equal to $1 - |b - v|$ where $b$ was the brightness value of Fido's LED ranging from 0 to 1 and $v$ was the intensity of visible light that Fido detected ranging from 0 to 1.

``Float,'' Fido's second task, challenged our learning implementation to direct a robot to point.
Each time it was told to select an action, Fido specified the robot's vertical and horizontal velocity between +30 and -30 pixels.
This emulates a holonomic drive systems, where motor outputs directly correlate to movement on the $x$ and $y$ axes.
At the start of each trial, Fido and the point were placed randomly on a boundless plane within 768 pixels of one another.
Fido was fed the ratio of its $x$ displacement to its $y$ displacement from its target point as the state.
Reward was calculated as the difference between Fido's distance away from the target point before performing the action and Fido's distance from the target point after performing the action.
Fido completed each trial when it was within 60 pixels of the point.

Fido's next task, nicknamed ``Drive,'' required that it direct a robot to point by controlling the motors of a differential drive system.
At the start of each trial, Fido and the point were placed randomly on a boundless plane within 768 pixels of one another.
Fido was fed the ratio of its $x$ displacement to its $y$ displacement from its target point as well as its rotation.
Reward was calculated as the difference between Fido's distance away from the target point before performing the action and Fido's distance from the target point after performing the action.
Fido completed each trial when it was within 60 pixels of the point.

Fido's last challenge, called ``Follow,'' was to perform the classic robotic task of line following by controlling the motors of the simulator's differential drive system.
At the start of each trial, Fido was placed on  a line with a random rotation.
Fido was fed the ratio of its $x$ displacement to its $y$ displacement from the closest point on the line as well as its rotation.
Reward was calculated as the difference between Fido's distance away from the line before performing the action and Fido's distance from the line after performing the action.
Fido completed each trial when it had stayed within 60 pixels of the line for 10 consecutive actions.

\subsubsection{Simulation Findings}

Each task was run 400 times to gather the data show in Table \ref{tab:data}.
The reward iterations values shown in the above data table were the medians of the data collected.
The median is shown instead of the mean to discount a few large outliers that were present in data.
The time data shown above is the mean of the data collected.

\begin{table}[ht]
	\centering
	\begin{tabular}{@{}lccc@{}}
		\toprule
		Task        & Learning Iterations & Action Selection (ms) & Training Time (ms) \\ \midrule
		Flash       & 6                   & 0.                 & 28                    \\
		Float       & 14                  & 1                  & 63                    \\
		Drive       & 16                  & 1                  & 94                    \\
		Line Follow & 10                  & 0.                 & 90.				   \\ \bottomrule
	\end{tabular}
	\caption{Number of Learning Iterations, Action Selection Time, and Training Time Per Iteration for Fido Simulation Tasks}
	\label{tab:data}
\end{table}

The data collected through the simulator demonstrates the prowess of the Fido control system with computer-delegated reward in multiple configurations and situations.
Fido was able to master both a holonomic and differential-drive motor control system (the ``Float'' and ``Drive'' tasks), proving its hardware agnostic capabilities.
All tasks showed low numbers of reward iterations and low latency, allowing Fido to learn quickly and efficiently.
The task that was most difficult for Fido, ``Drive,'' took a median of 16 reward iterations, well within the patience of a human.

\subsection{Results in Hardware}

Data was next collected from Fido's hardware implementations to gage performance with human given feedback in real world situations.
Reward was administered through the mobile application previously discussed.

\subsubsection{Thing One Results}

The first task to be given to Thing One was to stay still.
As one would expect, Fido's sole responsibility for this task was not to move.
Fido was administered positive reward when it didn't move, and negative reward when it did.
Next, Thing One was tasked with driving to a point.
At the start of each trial, Fido and the point were placed randomly on a smooth, hardwood surface within 0.75 meters of one another.
Fido was told the ratio of its $x$ displacement to its $y$ displacement from its target point as well as its rotation.
Every fourth action that Fido made was chosen as a reward iteration, and Fido was given a reward value corresponding to its last action.
This reward value was chosen by the tester based on whether Fido moved toward the point or not.
Fido completed each trial when it was within 10 cm of the point.

25 trials were done for each task to gather the data shown in Table \ref{tab:data2}.
As with the simulation, reward iterations values shown are the medians of the data collected while time data shown is the mean of the data collected.

\begin{table}[ht]
	\centering
	\begin{tabular}{@{}lccc@{}}
		\toprule
		Task             & Learning Iterations & Action Selection (ms) & Training Time (ms) \\ \midrule
		Stay Still       & 3                   & 1                    & 43.5                  \\
		Drive to Point   & 18                  & 4                     & 65                  \\
	\end{tabular}
	\caption{Number of Learning Iterations, Action Selection Time, and Training Time Per Iteration for Thing One Tasks}
	\label{tab:data2}
\end{table}

Performance was slower on the hardware implementation than in simulation for the drive to point task due to the limited computation power of the Intel Edison compute model compared to modern desktop computers.
However, the results still demonstrated Fido's real world applicability through its ability to perform outside of a simulation environment.
